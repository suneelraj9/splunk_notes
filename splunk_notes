What is Splunk?

Splunk is like a super-smart detective that helps companies find clues and solve mysteries in their data. Imagine you have a massive library filled with books (your data), but you have no idea what's in them. Splunk reads all those books and helps you find exactly what you're looking for, quickly and efficiently.
Example

Let's say a company wants to know why its website keeps crashing. Splunk can sift through all the website's logs and data, find the patterns or issues that are causing the crashes, and present them in an understandable way.
Why use Splunk?

    Find Information Quickly: Just like how you might use Google to find information on the internet, Splunk helps businesses search through massive amounts of data quickly.

    Create Reports and Visualizations: It's not just about finding data; Splunk can also make charts, graphs, and reports. It's like turning a messy room full of papers into neat, organized files.

    Monitor Systems: Splunk can keep an eye on things for you, like how well a website is running, and alert you if something goes wrong.

Example

A company uses Splunk to keep track of sales. When sales are higher or lower than usual, Splunk sends a notification to the sales team. They can then look into what's happening and make smart business decisions.
Overview of Splunk's Architecture

Splunk's architecture is like the different parts of a big factory that works together to build something useful.

    Forwarders: These are like the trucks that bring raw materials (data) into the factory (Splunk). They collect data from various sources like computers, servers, etc., and send it to the next stage.

    Indexers: Imagine these as the machines that take the raw materials (data) and organize them into a way that makes sense, like sorting different types of metals.

    Search Heads: This part is like the control room where you can ask Splunk what you want to know. It takes your questions (searches), looks through the organized data (indexes), and gives you the answers.

    Deployment Server: Think of this as the manager who makes sure everyone is doing their job correctly. It controls how the forwarders and other parts are set up and working.

    Cluster Master: This is like a supervisor making sure all the machines (indexers) are working together properly and efficiently.

Example

Think of a busy kitchen. The forwarders are the chefs preparing the ingredients, the indexers are the cooking stations organizing and cooking the food, the search heads are the waiters taking your order and bringing you your meal, the deployment server is the head chef making sure everything runs smoothly, and the cluster master is the manager making sure all parts of the kitchen are working together.

By understanding these different parts and how they work together, you can see how Splunk takes vast amounts of messy data and turns it into useful information that businesses can use to make smart decisions.

System Requirements

Before you can install Splunk, you need to make sure your computer or server meets certain requirements. It's like checking if you have enough room and the right tools before you build something.

    Processor: Your computer's brain. You'll need at least a 2 GHz CPU. It's like needing a good engine in a car.
    Memory (RAM): This is your computer's short-term memory. You'll need at least 8 GB, so Splunk has enough space to think and work.
    Disk Space: Your computer's storage or long-term memory. You'll need at least 20 GB of free space to store all the Splunk files.
    Operating System (OS): Splunk can run on different OS like Windows, Linux, or Mac. It's like being able to drive different types of cars.

Installing Splunk Enterprise on Different OS

Windows:

    Download: Get the Splunk installer from the official website.
    Run Installer: Double click on the downloaded file. It's like opening a box you just got.
    Follow Instructions: A wizard will guide you through the installation. Think of this as a friendly helper who tells you what to do step by step.

Linux:

    Download: Get the Splunk tar file from the official website.
    Extract Files: Open your terminal and use the command tar xvzf splunk-<version>-<build>-<platform>.tgz. It's like unzipping a suitcase.
    Run Installer: Navigate to the Splunk folder and run ./splunk start. It's like starting your car's engine.

Mac:

The process for Mac is similar to Linux. Follow the same steps and use the appropriate file for Mac.
Basic Configuration

Once Splunk is installed, you need to set it up the way you like, just like adjusting your seat and mirrors when you get into a new car.

    Start Splunk: Open your browser and go to http://localhost:8000. It's like going to a specific room in a big building.
    Login: The default username is admin and the password is changeme. It's like having a key to your own room.
    Change Password: You'll want to change the default password to something only you know. It's like changing the lock to make sure you're secure.
    Add Data: You can start adding data from your computer or network. Think of this as putting books on a shelf.

Example

Installing Splunk on a computer is like setting up a new kitchen appliance. First, you check if you have enough space and the right connections (system requirements). Then you follow the instructions to put it together and plug it in (installing). Finally, you set the time, temperature, and other settings so it works just the way you want (configuration).
Conclusion

Installing Splunk is like setting up a new gadget. You need to make sure you have what you need, follow the instructions to put it together, and then set it up the way you like. Once you have Splunk installed, you can start using it to search and analyze data, just like how a new gadget helps you in your daily life!

Introduction to SPL (Search Processing Language)

SPL is like a secret language that you and Splunk understand. It lets you ask Splunk specific questions about your data. Imagine you're a detective, and you need to ask just the right questions to find the clues you need. That's what SPL does for you.
Example

If you have logs from a website and you want to know how many people visited yesterday, you might use SPL to ask Splunk that exact question.
Basic Search Commands

Splunk's basic search commands are like the simple tools in a toolbox. You'll use these all the time.

    Searching for a Word or Phrase: Just type the word into the search bar. Like looking for a book in a library by its title.
        error: This will find all the logs with the word "error."

    Using AND, OR, NOT: You can combine words and phrases to make your search more specific.
        error AND login: This finds logs that contain both "error" and "login."

    Counting Things: You can count how often things happen.
        top limit=5 source: This shows the top 5 sources.

    Sorting Results: You can sort your results to see them in a specific order.
        sort - failures: This sorts the results by failures in descending order.

Time Ranges and Indexes

Imagine your data is a huge library of books. Time ranges and indexes are like the different sections and shelves that help you find what you're looking for.

    Time Ranges: This lets you search for data from a specific time.
        Last 15 minutes, 1 hour, 24 hours: You can pick common time ranges from a dropdown menu.
        Custom Time: You can also choose exactly the start and end times you want.

    Indexes: Indexes are like specific shelves or sections in your library of data.
        Search Specific Index: index=weblogs: This looks only in the "weblogs" section of your data.

Example

Say you want to find out how many errors happened on your website last Saturday. You'd use a time range to look only at last Saturday's data, and you might use an index to look only at the website logs. Then you'd use the basic search commands to look for the word "error" and count how many times it appears.
Conclusion

Searching with Splunk is like being a detective or a librarian. You use SPL to ask specific questions about your data. The basic search commands are your everyday tools. Time ranges and indexes help you narrow down where you're looking. With these tools, you can find just about anything you want to know about your data.

As you get more comfortable with these basics, you'll find that Splunk's search capabilities are very powerful. You can ask more and more complex questions and find very specific answers. It's like starting with simple puzzles and working your way up to solving big mysteries!

Sub-Searches

Sub-searches allow you to perform a search within a search. Think of it like looking inside a specific book for a particular chapter and then looking inside that chapter for a particular paragraph.

Example:

spl

index=weblogs [search index=weblogs error | top clientip]

This search first finds logs with "error" and then lists the top client IPs from those error logs.
Joining Searches

The join command lets you combine results from two different searches. Imagine it as joining two different puzzles to form a whole picture.

Example:

spl

index=weblogs status=200 | join type=outer clientip [search index=weblogs status=404]

This will show you both successful (200) and error (404) logs for each client IP.
Statistical Functions

You can use statistical functions to perform advanced mathematical operations on your data, like finding the average, sum, or standard deviation. It's like using a calculator to analyze your data.

Example:

spl

index=weblogs | stats avg(response_time) by clientip

This calculates the average response time for each client IP.
Transaction Command

The transaction command lets you group events that are part of the same transaction. Think of this as following a trail of breadcrumbs through your data.

Example:

spl

index=weblogs | transaction clientip startswith=login endswith=logout

This will group together all the events from login to logout for each client IP.

Time-based Functions

Time-based functions allow you to work with time in more complex ways. You can analyze patterns over time, break down data by time units, and more.

Example:

spl

index=weblogs | timechart count by status

This creates a chart showing the count of different status codes over time.
Macros

Macros allow you to save complex searches or functions to use later. Think of them as shortcuts that you can use to save time.

Example:
Create a macro to find all error logs:

spl

`error_logs`
Then use it in a search:

spl

index=weblogs `error_logs`

Conclusion

Splunk's advanced search capabilities allow you to delve deep into your data. Whether you're grouping related events, performing complex calculations, or breaking down patterns over time, these tools enable you to extract meaningful insights.

These advanced tools are like having a laboratory full of specialized instruments. With practice and experimentation, you can use them to explore your data in almost limitless ways, uncovering insights that can help you understand what's happening and why. Make sure to take advantage of the extensive documentation and community support available as you continue to explore these advanced features.

Understanding Fields

Fields are key-value pairs in your data. Think of a field as a label, like the title of a book, and the value as the specific information for that label, like the actual title of a book you own.

Example:
If you have a log entry: user=john action=login, "user" and "action" are fields, and "john" and "login" are the corresponding values.
Extracting Fields

Sometimes, the fields you want to work with are not automatically identified by Splunk. In these cases, you'll need to extract them manually.

    Using the Field Extractor:
        Navigate to a specific event in Splunk.
        Highlight the portion of data you want to extract.
        Follow the extraction wizard to create the field.

    Using SPL Commands:
        rex: The rex command allows you to define a regular expression to extract the field.
        Example: | rex "user=(?<username>\w+)" - This will extract the username from a log entry like user=john.

Using Fields in Searches

Fields are incredibly useful when searching, allowing you to filter, sort, and analyze data with great precision.

    Filtering by Field:
        Example: index=weblogs user=john - This will show all logs where the user field is "john."

    Sorting by Field:
        Example: index=weblogs | sort - date - This will sort the logs by the date field in descending order.

    Calculating Statistics by Field:
        Example: index=weblogs | stats count by status - This will count the number of occurrences for each status value.

    Using Eval to Manipulate Fields:
        eval: This command lets you perform calculations and create new fields.
        Example: index=weblogs | eval response_time_seconds=response_time/1000 - This will create a new field converting response time to seconds.

    Using Field Aliases:
        You can create an alias for a field to refer to it with a different name.
        Example: index=weblogs | fieldalias client=clientip - This lets you refer to the "clientip" field as "client."

Conclusion

Fields in Splunk are like the different types of information you can find in a detailed library catalog. By understanding, extracting, and using fields, you can navigate through vast amounts of data with great precision. Whether you're using built-in tools or SPL commands to work with fields, they are essential for advanced data analysis and visualization.

From filtering results to creating complex calculations, fields provide a wide array of functionalities to unlock insights from your data. The examples given are just the tip of the iceberg; as you continue to explore and experiment with fields, you'll uncover even more ways they can enhance your data analysis within Splunk.

splunk URL
**********
    Splunk Splexicon>
       https://docs.splunk.com/Splexicon
       

    Learning and Certification>
        Reg: Fund-2 -> 455-37024-360536-260-976765 
        https://education.splunk.com/user/learning/enrollments
	https://inter.viewcentral.com/reg/splunk/login 
	http://education.splunk.com/HowToEnroll
	 
    Splunk certifications>
        Please download your certificate for Splunk Infrastructure Overview 6.x (eLearning) by clicking below:
	https://www.splunk.training/getpdf.asp?data=53841E810C0197875BA3D81019786EAF19010197865A0C5810197865A0C581019786CABEA01019782C460A01019785282870101978619A67810197867A3F4810197868A58C0101978619A67810197876BBD50101978619A67810197872B5770101978619A6781019786AA8BB010197875BA3D81019782C460A010197873B70E810197875BA3D81019786EAF19010197865A0C5810197865A0C581019786CABEA01019782E4939010197872B5770101978619A67810197867A3F4810197868A58C0101978619A67810197876BBD50101978619A67810197872B5770101978619A6781019786AA8BB010197875BA3D81019784065E0010197866A25D010197869A723810197872B577010197873B70E810197874B8A60101978649F2E0101978619A67810197874B8A60101978619A6781019782E49390101978639D9681019786FB0B081019786DAD8181019782C460A0101978314DFF8101978304C680101978395ABB81019782C460A0101978436AA681019782C460A0101978598DAB81019782C460A010197853841E810197870B24801019786CABEA010197875BA3D81019786EAF1901019786BAA5281019782D47A1810197865A0C581019784C78FA010197865A0C58101978619A67810197872B57701019786EAF19010197869A72381019786EAF19010197867A3F481019782C460A0101978304C680101978324F9701019782F4AD08101978304C6801019783452C601019782F4AD08101978324F970101978304C680101978314DFF8101978395ABB8101978|25CD3F|15B3C7
        Some email clients may break that long link. If that link doesn't work, you can also use this one: https://splunk.training/A5XRAMXQ
	
	Please download your certificate for Splunk 7.x Fundamentals Part 1 (eLearning) by clicking below:
        https://www.splunk.training/getpdf.asp?data=67943B010C3F79092024D013F7908945FE013F7907E0ABD013F7907E0ABD013F79086C70C013F79036E8CC013F7906654C2013F790790CD9013F7908089AF013F79081C928013F790790CD9013F7909341C6013F790790CD9013F7908E43E2013F790790CD9013F79084481A013F79092024D013F79036E8CC013F7908F835B013F79092024D013F7908945FE013F7907E0ABD013F7907E0ABD013F79086C70C013F7903967BE013F7908E43E2013F790790CD9013F7908089AF013F79081C928013F790790CD9013F7909341C6013F790790CD9013F7908E43E2013F790790CD9013F79084481A013F79092024D013F7904FDE40013F7907F4A36013F7908308A1013F7908E43E2013F7908F835B013F79090C2D4013F7907CCB44013F790790CD9013F79090C2D4013F790790CD9013F7903967BE013F7907B8BCB013F7908A8577013F790880685013F79036E8CC013F79040E494013F7903FA51B013F7903FA51B013F79045E278013F7903E65A2013F790436386013F7903D2629013F7903BE6B0013F79036E8CC013F790539CAB013F79036E8CC013F7906F1111013F79036E8CC013F79067943B013F7908BC4F0013F79086C70C013F79092024D013F7908945FE013F790858793013F790382845013F7907E0ABD013F7905ED7EC013F7907E0ABD013F790790CD9013F7908E43E2013F7908945FE013F7908308A1013F7908945FE013F7908089AF013F79036E8CC013F7903BE6B0013F7903D2629013F7903AA737013F7903FA51B013F7903D2629013F7903AA737013F7903E65A2013F7903BE6B0013F7903D2629013F7904721F1013F790|31DAFB|1DE36B
        Some email clients may break that long link. If that link doesn't work, you can also use this one: https://splunk.training/B4NWBFHX
	
	Certificate for Splunk 7.X Fundamentals Part 2 (IOD):
	https://www.splunk.training/getpdf.asp?data=62964D510C301378AF8E2313013782A85A213013777F7AB313013777F7AB313013780483341301373443574130137616639E13013773375D71301377A57D211301377B87E5813013773375D71301378C28F5A13013773375D71301378768A7E13013773375D71301377DE80C61301378AF8E2313013734435741301378898BB51301378AF8E2313013782A85A213013777F7AB313013777F7AB3130137804833413013736A37E21301378768A7E13013773375D71301377A57D211301377B87E5813013773375D71301378C28F5A13013773375D71301378768A7E13013773375D71301377DE80C61301378AF8E231301374C04DC01301377927BEA1301377CB7F8F1301378768A7E1301378898BB513013789C8CEC13013776C797C13013773375D713013789C8CEC13013773375D713013736A37E2130137759784513013783D86D9130137817846B13013734435741301373DC3F2C1301373C93DF51301373C93DF513013742844081301373B63CBE130137402419A1301373A33B871301373DC3F2C13013734435741301374F95165130137344357413013769B6C1F130137344357413013762964D5130137850881013013780483341301378AF8E2313013782A85A21301377F181FD13013735736AB13013777F7AB31301375A45C5413013777F7AB313013773375D71301378768A7E13013782A85A21301377CB7F8F13013782A85A21301377A57D2113013734435741301373903A501301373B63CBE13013737D39191301373B63CBE130137428440813013737D39191301373B63CBE1301373903A501301373A33B8713013743B453F130137|2724CE|142397
	
	Certificate for Splunk User Behavior Analytics (eLearning):
	https://www.splunk.training/getpdf.asp?data=97577E010CD2CA0D5565201D2CA0C892CC01D2CA0B829B201D2CA0B829B201D2CA0C4ED3801D2CA0503AB801D2CA09584B401D2CA0B0DE8A01D2CA0BBCF4601D2CA0BDA21001D2CA0B0DE8A01D2CA0D7291C01D2CA0B0DE8A01D2CA0CFDDF401D2CA0B0DE8A01D2CA0C147A401D2CA0D5565201D2CA0503AB801D2CA0D1B0BE01D2CA0D5565201D2CA0C892CC01D2CA0B829B201D2CA0B829B201D2CA0C4ED3801D2CA053E04C01D2CA0CFDDF401D2CA0B0DE8A01D2CA0BBCF4601D2CA0BDA21001D2CA0B0DE8A01D2CA0D7291C01D2CA0B0DE8A01D2CA0CFDDF401D2CA0B0DE8A01D2CA0C147A401D2CA0D5565201D2CA074B28001D2CA0B9FC7C01D2CA0BF74DA01D2CA0CFDDF401D2CA0D1B0BE01D2CA0D3838801D2CA0B656E801D2CA0B0DE8A01D2CA0D3838801D2CA0B0DE8A01D2CA053E04C01D2CA0B4841E01D2CA0CA659601D2CA0C6C00201D2CA0503AB801D2CA05ED10801D2CA05CFE3E01D2CA05CFE3E01D2CA0661C3001D2CA05B2B7401D2CA060A3D201D2CA0661C3001D2CA067EEFA01D2CA0503AB801D2CA07A2ADE01D2CA0503AB801D2CA0A2483A01D2CA0503AB801D2CA097577E01D2CA0CC386001D2CA0C4ED3801D2CA0D5565201D2CA0C892CC01D2CA0C31A6E01D2CA0520D8201D2CA0B829B201D2CA08A93F801D2CA0B829B201D2CA0B0DE8A01D2CA0CFDDF401D2CA0C892CC01D2CA0BF74DA01D2CA0C892CC01D2CA0BBCF4601D2CA0503AB801D2CA05785E001D2CA05B2B7401D2CA055B31601D2CA05B2B7401D2CA064496601D2CA055B31601D2CA05B2B7401D2CA05785E001D2CA05958AA01D2CA067EEFA01D2CA0|30D869|13ABC9

 
    Enrolling Certification>
         1. Need to submit form if it is first time attempting splunk certification
      	 2. https://www.splunk.com/en_us/training/pearson-vue-registration-form.html
	 3. After getting splunk-id from splunk register in pearson
	 4. Create Pearson VUE account: www.PearsonVUE.com/splunk.
	 
	 Please see our registration tutorial for exam registration assistance: 
	   https://urldefense.proofpoint.com/v2/url?u=https-3A__www.splunk.com_pdfs_training_Exam-2DRegistration-2DTutorial.pdf&d=DwIFEA&c=ewHkv9vLloTwhsKn5d4bTdoqsmBfyfooQX5O7EQLv5TtBZ1CwcvjU063xndfqI8U&r=teKhIJqRQKTo38Oj0ZXd69ZhkK7rIrLI0q14TUfFetM&m=PFVXUXMQ6OBfGS12UCS2eI3HFS5ZIeRixAok4_lfobY&s=YeEZxj189a5rr6aBn-BaqDpIl-0H7TtkVv1m7fsMSOw&e=
	   
	 Looking for information on the new program? Check the following links for more information:
	   Splunk Certification Candidate Handbook - 
	    https://urldefense.proofpoint.com/v2/url?u=https-3A__www.splunk.com_pdfs_training_Splunk-2DCertification-2DCandidate-2DHandbook.pdf&d=DwIFEA&c=ewHkv9vLloTwhsKn5d4bTdoqsmBfyfooQX5O7EQLv5TtBZ1CwcvjU063xndfqI8U&r=teKhIJqRQKTo38Oj0ZXd69ZhkK7rIrLI0q14TUfFetM&m=PFVXUXMQ6OBfGS12UCS2eI3HFS5ZIeRixAok4_lfobY&s=k7wJ3jekf8iowqDZ3zNVdJ8-_IwtlVO65GXYf_uvn2U&e=
	    
	 Splunk Certification + Training 
	   https://urldefense.proofpoint.com/v2/url?u=http-3A__Splunk.com_Education&d=DwIFEA&c=ewHkv9vLloTwhsKn5d4bTdoqsmBfyfooQX5O7EQLv5TtBZ1CwcvjU063xndfqI8U&r=teKhIJqRQKTo38Oj0ZXd69ZhkK7rIrLI0q14TUfFetM&m=PFVXUXMQ6OBfGS12UCS2eI3HFS5ZIeRixAok4_lfobY&s=YDuzeC11UAoIPkuqTRHUYtgEVwfhzQ04_WdfTlkrdrw&e
	   
     Splunk Storage Sizing>
       	 https://splunk-sizing.appspot.com/
	 
     Cheat Sheet>
         https://lzone.de/cheat-sheet/Splun 

     Base app>
	  Dashboard examples -> https://splunkbase.splunk.com/app/1603/

     known issues
	  THP Disable -> https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP

     memory overcommiting
	  https://docs.splunk.com/Documentation/Splunk/7.2.3/ReleaseNotes/LinuxmemoryovercommittingandSplunkcrashes
   
     install splunk on windows through cli
	  https://docs.splunk.com/Documentation/Splunk/latest/Installation/InstallonWindowsviathecommandline

     Secure Splunk Web with your own certificate
	  https://docs.splunk.com/Documentation/Splunk/latest/Security/SecureSplunkWebusingasignedcertificate

     Components and the data pipeline
	  https://docs.splunk.com/Documentation/Splunk/latest/Deploy/Componentsofadistributedenvironment

     Other manuals for the Splunk platform administrator
	  https://docs.splunk.com/Documentation/Splunk/latest/Admin/Whatsinthismanual

     Archive indexed data
	  https://docs.splunk.com/Documentation/Splunk/latest/Indexer/Automatearchiving

     Restore archived indexed data
	  https://docs.splunk.com/Documentation/Splunk/latest/Indexer/Restorearchiveddata

     indexes.conf.spec
	  https://docs.splunk.com/Documentation/Splunk/latest/admin/Indexesconf#indexes.conf.spec

     Authentication methods
	  https://docs.splunk.com/Documentation/Splunk/latest/Security/Hardeningstandards 

     Plan a deployment
	  https://docs.splunk.com/Documentation/Splunk/6.2.1/Updating/Planadeployment

     Deploy a heavy forwarder
	  https://docs.splunk.com/Documentation/Splunk/latest/Forwarding/Deployaheavyforwarder

     About securing data from forwarders
	  https://docs.splunk.com/Documentation/Splunk/latest/Security/Aboutsecuringdatafromforwarders

     About the Monitoring Console
	  https://docs.splunk.com/Documentation/Splunk/latest/DMC/DMCoverview
  

splunk queries
***************
	> write a search to pull the OS distribution of all hosts
	   index=_internal fwdType="*" | dedup hostname | stats count by os, version

	> To check the process states
	   index=os sourcetype=ps rhnsd|dedup host|stats count as runningCount|eval rhnsdMissing=(10-runningCount)| table rhnsdMissing

	> Combine three different source types(CPU,Memory, Network Utilization as perecentage) 
	  index=xyz (sourcetype=CPUtime OR sourcetype=Memory OR sourcetype=Localnetwork)  (counter="% Processor Time" OR counter="Committed Bytes" OR counter="Bytes Received/Sec") | chart  avg(Value) as values over host by counter

	> create and trigger an alert if the CPU usage is constantly 100% for the past 10 minutes
	  index=* host=zzzz sourcetype="Perfmon:CPU" source="Perfmon:CPU" counter="% Processor Time" | timechart span=10m limit=0 avg(Value) as "% of CPU Usage"

	> 



splunk forwarder
****************
	 >> Enable the splunk service during boot:
	   /opt/splunkforwarder/bin/splunk enable boot-start
	 >> Splunk add forwarder through CLI
	   /opt/splunkforwarder/bin/splunk add forward_server 10.x.x.x:9997
	 >> Add monitor of the log file
	   /opt/splunkforwarder/bin/splunk add monitor /opt/log/www1
	 >> 
 
 hardware errors key words
 *************************
 
	 "hardware error" AND 
 
Splunk tutorial
***************
	
Module - 9
***********

	 > top
	   index=sales sourcetype=vendor_sales 
	   | top vendor

	   | top vendor limit=10

	   | top vendor product_name limit=0

		   clauses used in top
		   limit=int
		   countfield=string
		   percentfield=string
		   showcount=True/False
		   showperc=True/False
		   showother=True/False
		   otherstr=string

	   | top vendor limit=5 showperc=False countfield="Number of Sales" useother=True

	     //useother - vendor not listed in top 5

	     // by clause
	     / top product_name by vendor limit=3 countfield="Number of Sales" showperc=False
   

	> rare same options like top command

	> stats 
	  stats functions(count,distinct count, sum, average, list, values)

	  index=sales sourcetype=vendor_sales
	  | stats count

	  | stats count as "Total Sells By Vendors"

	  | stats count as "Total Sells By Vendors" by product_name

	  | stats count as "Total Sells By Vendors" by product_name,categoryid,sale_price

	  //passing field to the count

	  index=web sourcetype=access_combined
	  | stats count(action) as ActionEvents

	  //Adding another count
	  index=web sourcetype=access_combined
	  | stats count(action) as ActionEvents,count as "Total Events"

	  //Distict count(dc) or dc interchangable
	  index=sales sourcetype=vendor_sales
	  | stats distinct_count(product_name) as "Number of games for sale by vendors" by sale_price


	  //sum usage
	  index=sales sourcetype=vendor_sales
	  | stats sum(price) as "Gross Sales" by product_name

	  | stats count as "Units Sold" sum(price) as "Gross Sales" by product_name

	  //avg usage
	  index=sales sourcetype=vendor_sales
	  | stats avg(sale_price) as "Average Selling Price" by product_name
	  | stats count as "Units Sold" 
	    avg(price) as "Average Retail Price"
	    avg(sale_price) as "Avg Selling Price" by product_name

	  //List Usage
	  index=bcgassets sourcetype=asset_list
	  | stats list(Asset) as "company assets" by Employee

	  //value Usage - only returns unique value
	  index=network sourcetype=cisco_wsa_squid
	  | stats values(s_hostname) by cs_username
  
	lookup
	*******

		add additional fields and numbers

		 > Need to upload the file locally from the desktop
		 > | inputlookup http_status.csv

		 Define lookup
		 *************
		 Go to settings-> lookup
		 select lookup definitions

		 Use lookup search query
		 ***********************
		 index=web sourcetype=access_combined NOT status=200
		 | lookup http_status code as status,
		 OUTPUT code as "HTTP Code",
		 description as "HTTP Description"

		 > Do not want override we can use 'OUTPUTNEW' class
		 OUTPUTNEW code as "HTTP Code"

		 index=web sourcetype=access_combined NOT status=200
		 | lookup http_status code as status,
		 OUTPUTNEW code as "HTTP Code",
		 description as "HTTP Description"
		 | table host, "HTTP Code", "HTTP Description"

		 >>>Automatic lookup
		 setting -> lookup
 
 
>>Fundamentails (Part-2)
************************
   Buckets - Configurable maximum timespan, size
   ********
	   HOT - WRITABLE - Reaches max size, time span reached, indexer restarted
	   WARM - Hot bucket rooling to warm bucket.Closed,renameed, changed to read only (ex:db_youngesttimestamp_oldtimestamp)
           COLD - Warm bucket rolling to cold bucket after reaching max size and time span
				It stored in different locations which is less spped hard drive to cost optimize

	  > Prioroty of search options to get best search results
	         time
		 index
		 source
		 host
		 sourcetype
	 
  Visualization Intro
  ********************

	index=sales sourcetype=vendor_sales
	| stats count(inline) as "UnitsSold" by product_name
	
	index=sales sourcetype=vendor_sales	
	| timechart count(inline) as "Units Sold" by product_name limit=5
	
	Chart
	*****
	  over, by class
	  index=web sourcetype=access_combined status>299
	  | chart count over status by host
	  
	  > Two options can be used under x-axis unlike stats command
	    index=web sourcetype=access_combined status>299
	    | chart count by status,host
		
	   > index=web sourcetype=access_combined status>299
	    | chart count over host by product_name usenull=f useother=f limit=5
		  
	 Timechart
         *********
	    >index=sales sourcetype=vendor_sales
	     | timechart count 
		
	     >index=sales sourcetype=vendor_sales
	     | timechart count by product_name
		
	      stats function can be applicable to timechart command - sum(price), avg(price)
		
		>index=sales sourcetype=vendor_sales
		| timechart sum(price) by product_name [we can user limit, usenull,useother]
		
		>index=sales sourcetype=vendor_sales
		| timechart span=12hr sum(price) by product_name

	  Scatter chart - shows the relation between two distinct values
	  *************
		Ex: index=sales sourcetype=vendor_sales VendorID>=4000 AND VendorID<=8999
		    | stats count as "Units Sold", sum(sale_price) as "Total Sales" by VendorCountry,product_name

	  Bubble chart - we can view third dimension of data
	  ************
		 Ex: index=sales sourcetype=vendor_sales
		    |VendorID >=4000 AND VendorID <= 8999
		    |stats sum(sale_price) as "Total", count as "#Sold"
		    by date_hour | sort date_hour
		    |rename date_hour as "Time" | table "Time" "#Sold" "Total"

   Advanced visualization
   ***********************
	  iplocation - city
             	       country
		       Region, 
		       Latitude, 
		       Longitude

	     example: index=security sourcetype=linux_secure action=success src_ip!=10.*       
		      | iplocation src_ip


          Using Geostats command
          **********************
 	      Aggregates geographical data for use on a map visualization
	    
		    example: index=sales sourcetype=vendor_sales
			     | geostats latfield=VendorLatitude longfield=VendorLongitude count

			or count by product_name globallimit=10

	  Using geostats with iplocation
	  *******************************

		   index=sales sourcetype=linux_secure action=success src_ip!=10.*
		   | iplocation src_ip
		   | geostats latfield=lat longfield=lon count

          choropleth Maps
          ***************

		    Will need Keyhole Markup Language File(.kmz)
		    Splunk ships two files: >> geo_us_states.kmz
					    >> geo_countries.kmz

		   Ex: index=sales sourcetype=vendor_sales VendorID>=5000 AND VendorID<=5055 
			| stats count as Sales by VendorCountry
			| geom geo_countries featureField=VendorCountry

	  singlevalue visualization
	  **************************

		    index=web sourcetype=access_combined action=purchase 
			| stats sum(price) as total

	  trendline commmand
          ******************
		     index=web sourcetype=access_combined action=purchase status=200
			| timechart sum(price) as sales
			| trendline wma2(sales) as trend

		      2-> Two days average value took it out
		    Trendtype:sma -> sum moving average
			      ema -> exponential moving average
			      wma -> weighted moving average

		  chartoverlay will be useful when using trendline

field format
************

  customnize it in many ways using format option

	index=web sourcetype=access_combined file=*
	| chart sum(bytes) over host by file


Addtotals
**********

   index=web sourcetype=access_combined file=*
	| chart sum(bytes) over host by file
	| addtotals

       or | addtotals col=true label="Total" labelfield="host"

    fieldname="Total by host"


eval
*****

   supports - arithmetic, Concatenation, Boolean Operators supported

  ex: index=network sourcetype=cisco_wsa_squid 
	| stats sum(sc_bytes) as Bytes by usage
	| eval bandwidth =  Bytes/1024/102

     or  wrapping of calculation in round function
	| eval bandwidth = round(Bytes/1024/1024,2)
	| sort -bandwidth
	| rename bandwidth as "Bandwidth (MB)"
	| fields - Bytes
   
   eval mathematical functions
   ===========================
   index=web sourcetype=access_c* product_name=* action=purchase
	| stats sum(price) as total_list_price,	
	  sum(sale_price) as total_sale_price by product_name
	| eval discount= (total_list_price - total_sale_price/total_list_price)*100
	| sort - discount
	| eval discount = discount."%"
	
   eval convert values
   ===================
   index=web soucetype=access* product_name=* action=purchase
	| stats sum(price) as total_list_price,
	sum(sale_price) as total_sale_price by product_name
	|eval total_list_price = "$"+ tostring(total_list_price)

   fieldformat command (confusion)
   ===================
   
   Multiple eval commands
   ======================
   
   index=web sourcetype=access_combined price=*
	| stats values(price) as list_price,
	values(sale_price) as sale_price by product_name
	| eval current_discount = (((list_price-sale_price/list_price)*100)
	| eval new_discount = (current_discount - 5)
	| eval new_sale_price=list_price - (list_price * (new_discount/100))
	| eval price_change_revenue=(new_sale_price-sale_price)

   eval command if function
   ========================
   
    if (x,y,z)
      x-boolean expression
      y- if true it will evaluate
      z- if false it will evaluate
      y,z -  Must be in double quotes if not numerical

   ex: index=sales sourcetype=vendor_sales
	| eval SalesTerritory=
	if(VendorID<4000,"Northe America", "Rest of the World")
	| stats sum(price) as TotalRevenue by SalesTerritory

   eval command case function
   ==========================
   Allows you to take multiple boolean expressions and return the corresponding
   argument that is true

   ex: index=web sourcetype=access_combined
	| eval httpCategory=case(status>=200 AND status<300, "Success")

      We can add more booleans
     status>=300 AND status<400, "Redirect",
     status>=400 AND status<500, "Client Error",
     status>=500,"Server Error")

    using Eval with stats command
    ==============================
   index=web sourcetype=access_combined 
	| status count(eval(status<300)) as "Success",
	count(eval(status>=400 AND status<500)) as "Client Error",
	count(eval(status>500)) as "Server Error"

    where command
    ==============
    index=network sourcetype=cisco_wsa_squid
	| status count(eval(usage="Personal")) as Personal,
	count(eval(usage="Business")) as Business by username
	| where Personal>Business 
	| where username!="username"
	|sort -personal

    Fillnull command
    ================
    index=sales sourcetype=vendor_sales
	| chart sum(price) over product_name by VendorCountry
	| fillnull

     or using a value argument any string can be used
	fillnull value="nothing to see here"

Correlating events
********************

     Transaction Command
     ===================
	index=web sourcetype=access_combined
	| transaction clientip
        | table clientip,action,product_name
	
	index=web sourcetpe=access_combined
	|transaction clientip
	|timechart avg(duration)
 
     Transaction Command Definitions
     ===============================
      maxspan, maximum total time between earliest and latest events 
	maxpause, max total time between events
	startswith,  - terms, field values, evaluations
	endswith - terms, field values, evaluations

     Ex:
	index=web sourcetype=access_combined
	| transaction clientip
	startswith="addtocart" endswith="purchase"
	| table clientip,action,product_name

     Investigate with transactions
     =============================
	index=network sourcetype=cisco_esa REJECT
	| transaction mid dcid icid
	| search REJECT
	
     Examples:
     	index=web sourcetype=access_combined action=*
	| transaction JSESSIONID
	| table JSESSIONID, clientip, action
	
	index=web sourcetype=access_combined action=*
	| transaction JSESSIONID
	| table JSESSIONID, clientip, action
	| search action=purchase
	
	index=web sourcetype=access_combined action=*
	|  transaction JSESSIONID 
	| table JSESSIONID, clientip,duration,eventcount,action 
	| search action=purchase 
	| eval durationMinutes=round(duration/60,1) 
	|  where durationMinutes>1
	
	index=web sourcetype=access_combined
	| transaction clientip startswith=action="addtocart" endswith=action="purchase"
	| table clientip, JSESSIONID, product_name, action, duration, eventcount, price
	
	(index=network sourcetype=cisco_wsa_squid) OR
	(index=web sourcetype=access_combined) status>399
	| fields sourcetype, status
	| transaction status maxspan=5m
	| search sourcetype=access_combined AND sourcetype=cisco_wsa_squid
	| timechart count by status
	
	(index=network sourcetype=cisco_wsa_squid) OR (index=web sourcetype=access_combined)
	status>399
	| fields sourcetype, status
	| transaction status maxspan=5m
	| search sourcetype=access_combined AND sourcetype=cisco_wsa_squid 
	| timechart count by status 
	| addtotals 
	| search Total>4 
	| fields - Total
	
Naming Convention
*****************
 Six segmented field
	Group
	Type
	Platform
	Category
	Time
	Description
	
 As we are working for security-focused workflow action	
   Ex:OPS_WFA_Network_Security_na_IPwhoisAction
      Group- OPS
      Type- WFA (work flow action)
      Platform - Network
      Category - Security
      Not time nased used -NA
      IPwhoisAction - Description
      
 Give delegates to others
    Under setting in splunk -> All Configurations
      Right edge corner -> Reassign knowledge objects
      
 Splnk uses "CIM" (Common Information Model) normalizing objects


  knowledge object permissions
	> Private - Only that user
	> Specific App - Power user, Admin can grant read and write perissions
	> All Apps - admin can only one grant ead and write
	
Field extraction
******************

     Two types: 1 Regular expression
	2. delimiter which separated with comma
	
     Regular expression - Unstructured data and events that you want to extract field from
     Delimiters - Used when events contain fields separated by a character

     Extract with GUI three methods
     Extract with tools:

Field Aliases (Aliases and Calc Fields)
****************************************

	Sourcetype may have different field names for unique field

	ex: one sourcetype can have "username" as field
	    another sourcetype can have "user" as field

	Navigate to settings -> Fields
	  Destination App: search
	  Name: EmployeeUsers
	  Apply to: sourcetype
	  Field aliases: Username: Employee
	  
	Note: same way need to add one more field which having same unique field of sourcetype
	
     Examples:
     	Go to Settings > Fields > Field aliases. Create a field alias with the following values:
	— Destination app: search
	— Name: cisco_wsa_squid_aliases
	— Apply to: sourcetype
	— Named: cisco_wsa_squid
	— Field aliases: cs_username = user
	Return to the Search & Reporting app. Re-run your search and examine the user field and values
	
	Go to Settings > Fields > Calculated fields.
	Create a calculated field named sc_megabytes that converts the value of sc_bytes to MB with the
	following values:
	— Destination app: search
	— Apply to: sourcetype
	— Named: cisco_wsa_squid
	— Name: sc_megabytes
	— Eval expression: sc_bytes/(1024*1024)

Tags and Event types
*********************

	 Tag -> click event option -> Action option and tag the name of the field
	   after placing tags search by
	   index=security tag=SF
	   
	 Event type from Search
	 **********************
	  > After search -> Save as Event type
	    Name - purchase_strategy
	    Tags - purchase_strategy,sales
	    color- blue
	    priority - highest 
	    
	    RUnning a search with event type
	    index=web sourcetype=access_combined
	    | eventtype=purchase_strategy
	    
	  Event type builder
	  ******************
	  Event type can also obtain from event type builder
	  
	  index=web sourcetype=access_combined action=purchase categoryId*
	  
	  From information(i) event id we select Event Actions -> Build Event Type
	   
	 > select build event type

	   Which search would limit an "alert" tag to the "host" field?
		tag::host=alert

	   These allow you to categorize events based on search terms.
		Event Types

	   Tags are descriptive names for -> key value pairs
    
Macros
******

	  Reusable search strings or portions of search strings
	  Useful for frequest searches with complicated search syntax
	  store entire search strings
	  Time range independent
	   pass arguments to the search
  
	  create macro
	  =============

	  Ex: index=sales sourcetype=vendor_sales
		| stats sum(sale_price) as total_sales by Vendor
		|eval total_sales = "$" + tostring(round(total_sales,2), "commas")

	   settings -> advanced search -> add new macro
	   place it over there in Definition -> eval total_sales = "$" + tostring(round(total_sales,2), "commas")

	   index=sales sourcetype=vendor_sales
	   | stats sum(sale_price) as total_sales by Vendor
	   |'convertUSD'
		
	   Note: Using backtik character will indicate to the splunk it is macro definition

	   Macro argument
	   ==============

	   Multiple arguments can use during different search options
	   Setting-> Advanced search-> create macro
	   Name - convertUSD(1)
	   eval $moolah$ = "$" + tostring(round($moolah$,2),"commas")
	   Argument field -> moolah
	   
           Example:
	     index=sales sourcetype=vendor_sales
	     | stats sum(sale_price) as total_sales by Vendor
	     |'convertUSD(total_sales)'
	     
	  > Expanded search -> Ctl+Shift+E
	  
Workflow actions
*****************
      Create links to interact with external resources or narrow research
      
    Create GET  
      Navigate to settings -> Fields -> Click Work flow action
      
        Destination app-> search
	Name  -> OPS_WFA_Network_Security_ns_GetWhoIs
	Label -> Get Whois for $src_ip$
	Apply only to the following fields -> src_ip
	Apply only to the following event types -> <if balnk> Indicate applicable to all
	Show action in -> Both (Event and field)
	Action type -> link
	
	URI -> http://whois.domaintools.com/$src_ip$
	Open link in -> New window or Same Window
	Link method -> get
	
       To test our work flow action
         index=security sourcetype=linux_secure failed root src_ip>11
	 
     Create POST
        Navigate to settings -> Fields -> Click Work flow action
	Name  -> OPS_WFA_Network_Security_na_TicketPost
	Label -> Create ticket
	Apply only to the following field -> status
	Show action in -> Event menu
	Action type -> link
      
        Link configuration
	  URI -> http://52.3.246.206
	  Open link in -> New window
	  Link method -> post
	 
	  Post arguments -> summary = $status$error$host$
	  		    priority = urgetn
			    occurred=$_time$
			    environment=$host$
			    details=$_raw$
			   
	 Create search
	 
Data models
************
     Consits of
       Events
       Searches
       Transactions
       
     Data model has the framework, pivot has the interface to data model
       
     Data model create
       Navigate to settings -> Data model
       
       New Data Model
         Title - SALES_DM_Web_requests_na_Webrequests
	 ID - SALES_DM_Web_requests_na_WebRequests
	 App - search & reporting
	 
       Under datasets>Root datasets
       
         First need to create root event 
       
         Drop dow of add dataset - Root event > enable you to create hierarchies based on a set of events and are the most used type
	 			   Root search > Build hierarchies from transporming search 
         Note: Avoid using root search
	 
	 Add event dataset  
	   Dataset name - web_requests
	   Dataset ID - web_requests
	   Constraints -> index=web AND sourcetype=access_combined
	   
	 > After creating root event you can can see root transaction, root child along with root event, root search
	   Root transaction -> objects allow you to create datasets from groups of related events that span time.
	   		       They use an existing object from our data hierarchy to group on
	 
	 > We want to include multiple fields select Add field (drop down menu->Auto extra, Eval exp, Look up, Regul exp, Geo IP)
	 
         Using eval>
	   Example Using eval expression users can view it on day basis
	    strftime(_time,"%m:%d:%A")
	    We can create date field -> Field name->Day, Display name->Day  
	  
       Child Datasets>
	 Add Child dataset
	   Dataset Name - Successful requests
	   Dataset ID - Successful_requests
	   
	    Inherit from -> field filled up with default dataset
	    
	    Additional constraints>
	      status=200
	      >
       Using transactions>
          practise it
       Data model search>
          | pivot <data_model_name> <data_set_name> <search_terms> <commands
       Managing data models>
         Datamodels can download and use it in our dash board
       	  Accelarating data model - It can make searches faster and more efficient
	   https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/Acceleratedatamodels
	      
	      
Common information model(CIM)
*****************************
https://docs.splunk.com/Documentation/CIM/latest/User/Howtousethesereferencetables

  Normalized different occurences to a shared structure
  
  >Maps all data to defined method
  >Normalizes to common language
  
  Data can be normalized at index time/search time
  
  >CIM schema can be used for using
   -field extractions
   -aliase
   -eventtypes
   -tags
   
  >Knowledge objects can be shared globally across all apps
  >Splunk premium solutions rely heavily on CIM
  
  CIM Addon
  *********
   >validate indexed data
   >normalize data
   >improve performance
   
   - Addon is free 
   - No additional indexing
   - Doesn't affect license
   
  > Addon will only install on search head/single instance of deployment of splunk

